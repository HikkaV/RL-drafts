{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Description:\n",
      "        A pole is attached by an un-actuated joint to a cart, which moves along\n",
      "        a frictionless track. The pendulum starts upright, and the goal is to\n",
      "        prevent it from falling over by increasing and reducing the cart's\n",
      "        velocity.\n",
      "\n",
      "    Source:\n",
      "        This environment corresponds to the version of the cart-pole problem\n",
      "        described by Barto, Sutton, and Anderson\n",
      "\n",
      "    Observation:\n",
      "        Type: Box(4)\n",
      "        Num\tObservation               Min             Max\n",
      "        0\tCart Position             -4.8            4.8\n",
      "        1\tCart Velocity             -Inf            Inf\n",
      "        2\tPole Angle                -24 deg         24 deg\n",
      "        3\tPole Velocity At Tip      -Inf            Inf\n",
      "\n",
      "    Actions:\n",
      "        Type: Discrete(2)\n",
      "        Num\tAction\n",
      "        0\tPush cart to the left\n",
      "        1\tPush cart to the right\n",
      "\n",
      "        Note: The amount the velocity that is reduced or increased is not\n",
      "        fixed; it depends on the angle the pole is pointing. This is because\n",
      "        the center of gravity of the pole increases the amount of energy needed\n",
      "        to move the cart underneath it\n",
      "\n",
      "    Reward:\n",
      "        Reward is 1 for every step taken, including the termination step\n",
      "\n",
      "    Starting State:\n",
      "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
      "\n",
      "    Episode Termination:\n",
      "        Pole Angle is more than 12 degrees.\n",
      "        Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
      "        the display).\n",
      "        Episode length is greater than 200.\n",
      "        Solved Requirements:\n",
      "        Considered solved when the average reward is greater than or equal to\n",
      "        195.0 over 100 consecutive trials.\n",
      "    \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "print(env.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating NN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApproximateExpectedSarsa:\n",
    "    def __init__(self, state_dim, n_actions, epsilon=0.9, gamma=0.9,\n",
    "                eta=1e-4):\n",
    "        self.state_dim = state_dim\n",
    "        self.n_actions = n_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.eta = eta\n",
    "        self.model, self.sess = self.make_model()\n",
    "        self.initiate_placeholders()\n",
    "        self.train_step = self.make_graph()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def make_model(self):\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.InteractiveSession()\n",
    "        tf.keras.backend.set_session(sess)\n",
    "        model = Sequential([L.InputLayer(state_dim),\n",
    "                   L.Dense(200,activation='relu'),\n",
    "                   L.Dense(100, activation='relu'),\n",
    "                   L.Dense(self.n_actions)])\n",
    "        return model, sess\n",
    "    \n",
    "    def initiate_placeholders(self):\n",
    "        self.states_ph = tf.keras.backend.placeholder(dtype='float32', shape=(None,) + self.state_dim)\n",
    "        self.actions_ph = tf.keras.backend.placeholder(dtype='int32', shape=[None])\n",
    "        self.rewards_ph = tf.keras.backend.placeholder(dtype='float32', shape=[None])\n",
    "        self.next_states_ph = tf.keras.backend.placeholder(dtype='float32', shape=(None,) + self.state_dim)\n",
    "        self.is_done_ph = tf.keras.backend.placeholder(dtype='bool', shape=[None])\n",
    "    \n",
    "    def argmax(self,q_table):\n",
    "        max_value = np.max(q_table)\n",
    "        idxs = np.where(q_table==max_value)[0]\n",
    "        if idxs.shape[0]>1:\n",
    "            return np.random.choice(idxs)\n",
    "        else:\n",
    "            return np.argmax(q_table)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        q_values = self.model.predict(state[None])[0]\n",
    "    \n",
    "        if np.random.rand()<self.epsilon:\n",
    "              action = np.random.choice(n_actions)\n",
    "        else:\n",
    "              action = self.argmax(q_values)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def make_graph(self):\n",
    "        predicted_qvalues = self.model(self.states_ph)\n",
    "        predicted_qvalues_for_actions = tf.reduce_sum(predicted_qvalues * tf.one_hot(self.actions_ph, self.n_actions), \n",
    "                                                      axis=1)\n",
    "        predicted_next_qvalues = self.model(self.next_states_ph)\n",
    "        next_state_values = 1/self.n_actions * tf.reduce_sum(predicted_next_qvalues,axis=1)\n",
    "        target_qvalues_for_actions = self.rewards_ph+self.gamma*next_state_values\n",
    "        target_qvalues_for_actions = tf.where(self.is_done_ph, self.rewards_ph, target_qvalues_for_actions)\n",
    "        loss = (predicted_qvalues_for_actions - tf.stop_gradient(target_qvalues_for_actions)) ** 2\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return tf.train.AdamOptimizer(self.eta).minimize(loss)\n",
    "    \n",
    "    def update(self,s,a,r,next_s,done):\n",
    "        self.sess.run(self.train_step,{\n",
    "                self.states_ph: [s], self.actions_ph: [a], self.rewards_ph: [r], \n",
    "                self.next_states_ph: [next_s], self.is_done_ph: [done]\n",
    "            })\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=0.2\n",
    "gamma=0.99\n",
    "eta=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ApproximateExpectedSarsa(state_dim,n_actions,\n",
    "                            epsilon=epsilon, gamma=gamma, eta=eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(agent, env, train=False, t_max=1000):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        a = agent.get_action(s)       \n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        if train:\n",
    "            agent.update(s,a,r,next_s,done)\n",
    "        total_reward += r\n",
    "        s = next_s\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, epochs=1000,episodes=100,\n",
    "         epsilon_decay=True):\n",
    "    for i in range(epochs):\n",
    "        session_rewards = [generate_session(agent, env, train = True) for _ in range(episodes)]\n",
    "        print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), agent.epsilon))\n",
    "        if epsilon_decay:\n",
    "            agent.epsilon *= 0.99\n",
    "        assert epsilon >= 1e-4, \"Make sure epsilon is always nonzero during training\"\n",
    "\n",
    "        if np.mean(session_rewards) > 300:\n",
    "            print(\"You Win!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0\tmean reward = 10.390\tepsilon = 0.200\n",
      "epoch #1\tmean reward = 10.300\tepsilon = 0.200\n",
      "epoch #2\tmean reward = 10.660\tepsilon = 0.200\n",
      "epoch #3\tmean reward = 10.970\tepsilon = 0.200\n",
      "epoch #4\tmean reward = 11.620\tepsilon = 0.200\n",
      "epoch #5\tmean reward = 13.780\tepsilon = 0.200\n",
      "epoch #6\tmean reward = 10.270\tepsilon = 0.200\n",
      "epoch #7\tmean reward = 11.100\tepsilon = 0.200\n",
      "epoch #8\tmean reward = 10.530\tepsilon = 0.200\n",
      "epoch #9\tmean reward = 15.810\tepsilon = 0.200\n",
      "epoch #10\tmean reward = 16.820\tepsilon = 0.200\n",
      "epoch #11\tmean reward = 26.560\tepsilon = 0.200\n",
      "epoch #12\tmean reward = 46.420\tepsilon = 0.200\n",
      "epoch #13\tmean reward = 76.240\tepsilon = 0.200\n",
      "epoch #14\tmean reward = 70.610\tepsilon = 0.200\n",
      "epoch #15\tmean reward = 133.810\tepsilon = 0.200\n",
      "epoch #16\tmean reward = 236.880\tepsilon = 0.200\n",
      "epoch #17\tmean reward = 219.760\tepsilon = 0.200\n",
      "epoch #18\tmean reward = 140.110\tepsilon = 0.200\n",
      "epoch #19\tmean reward = 150.830\tepsilon = 0.200\n",
      "epoch #20\tmean reward = 180.160\tepsilon = 0.200\n",
      "epoch #21\tmean reward = 124.900\tepsilon = 0.200\n",
      "epoch #22\tmean reward = 216.630\tepsilon = 0.200\n",
      "epoch #23\tmean reward = 206.600\tepsilon = 0.200\n",
      "epoch #24\tmean reward = 249.010\tepsilon = 0.200\n",
      "epoch #25\tmean reward = 488.250\tepsilon = 0.200\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "train(agent,env,epsilon_decay=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showing agent performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"Step: %d %s\" % (step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of rewards : 50.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "steps = 50\n",
    "s = env.reset()\n",
    "rewards = 0\n",
    "for t in range(steps):\n",
    "    show_state(env,t)\n",
    "    env.render()\n",
    "    a = agent.get_action(s)\n",
    "    next_s, r, done, _ = env.step(a)\n",
    "    rewards+=r\n",
    "    s = next_s\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        break\n",
    "env.close()\n",
    "display.clear_output(wait=True)\n",
    "print(\"Sum of rewards : {}\".format(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
