{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Description:\n",
      "        A pole is attached by an un-actuated joint to a cart, which moves along\n",
      "        a frictionless track. The pendulum starts upright, and the goal is to\n",
      "        prevent it from falling over by increasing and reducing the cart's\n",
      "        velocity.\n",
      "\n",
      "    Source:\n",
      "        This environment corresponds to the version of the cart-pole problem\n",
      "        described by Barto, Sutton, and Anderson\n",
      "\n",
      "    Observation:\n",
      "        Type: Box(4)\n",
      "        Num\tObservation               Min             Max\n",
      "        0\tCart Position             -4.8            4.8\n",
      "        1\tCart Velocity             -Inf            Inf\n",
      "        2\tPole Angle                -24 deg         24 deg\n",
      "        3\tPole Velocity At Tip      -Inf            Inf\n",
      "\n",
      "    Actions:\n",
      "        Type: Discrete(2)\n",
      "        Num\tAction\n",
      "        0\tPush cart to the left\n",
      "        1\tPush cart to the right\n",
      "\n",
      "        Note: The amount the velocity that is reduced or increased is not\n",
      "        fixed; it depends on the angle the pole is pointing. This is because\n",
      "        the center of gravity of the pole increases the amount of energy needed\n",
      "        to move the cart underneath it\n",
      "\n",
      "    Reward:\n",
      "        Reward is 1 for every step taken, including the termination step\n",
      "\n",
      "    Starting State:\n",
      "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
      "\n",
      "    Episode Termination:\n",
      "        Pole Angle is more than 12 degrees.\n",
      "        Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
      "        the display).\n",
      "        Episode length is greater than 200.\n",
      "        Solved Requirements:\n",
      "        Considered solved when the average reward is greater than or equal to\n",
      "        195.0 over 100 consecutive trials.\n",
      "    \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "print(env.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApproximateQlearning:\n",
    "    def __init__(self, state_dim, n_actions, epsilon=0.9, gamma=0.9,\n",
    "                eta=1e-4):\n",
    "        self.state_dim = state_dim\n",
    "        self.n_actions = n_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.eta = eta\n",
    "        self.model, self.sess = self.make_model()\n",
    "        self.initiate_placeholders()\n",
    "        self.train_step = self.make_graph()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def make_model(self):\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.InteractiveSession()\n",
    "        tf.keras.backend.set_session(sess)\n",
    "        model = Sequential([L.InputLayer(state_dim),\n",
    "                   L.Dense(200,activation='relu'),\n",
    "                   L.Dense(100, activation='relu'),\n",
    "                   L.Dense(self.n_actions)])\n",
    "        return model, sess\n",
    "    \n",
    "    def initiate_placeholders(self):\n",
    "        self.states_ph = tf.keras.backend.placeholder(dtype='float32', shape=(None,) + self.state_dim)\n",
    "        self.actions_ph = tf.keras.backend.placeholder(dtype='int32', shape=[None])\n",
    "        self.rewards_ph = tf.keras.backend.placeholder(dtype='float32', shape=[None])\n",
    "        self.next_states_ph = tf.keras.backend.placeholder(dtype='float32', shape=(None,) + self.state_dim)\n",
    "        self.is_done_ph = tf.keras.backend.placeholder(dtype='bool', shape=[None])\n",
    "    \n",
    "    def argmax(self,q_table):\n",
    "        max_value = np.max(q_table)\n",
    "        idxs = np.where(q_table==max_value)[0]\n",
    "        if idxs.shape[0]>1:\n",
    "            return np.random.choice(idxs)\n",
    "        else:\n",
    "            return np.argmax(q_table)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        q_values = self.model.predict(state[None])[0]\n",
    "    \n",
    "        if np.random.rand()<self.epsilon:\n",
    "              action = np.random.choice(n_actions)\n",
    "        else:\n",
    "              action = self.argmax(q_values)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def make_graph(self):\n",
    "        predicted_qvalues = self.model(self.states_ph)\n",
    "        predicted_qvalues_for_actions = tf.reduce_sum(predicted_qvalues * tf.one_hot(self.actions_ph, self.n_actions), \n",
    "                                                      axis=1)\n",
    "        predicted_next_qvalues = self.model(self.next_states_ph)\n",
    "        next_state_values = tf.reduce_max(predicted_next_qvalues,axis=1)\n",
    "        target_qvalues_for_actions = self.rewards_ph+self.gamma*next_state_values\n",
    "        target_qvalues_for_actions = tf.where(self.is_done_ph, self.rewards_ph, target_qvalues_for_actions)\n",
    "        loss = (predicted_qvalues_for_actions - tf.stop_gradient(target_qvalues_for_actions)) ** 2\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return tf.train.AdamOptimizer(self.eta).minimize(loss)\n",
    "    \n",
    "    def update(self,s,a,r,next_s,done):\n",
    "        self.sess.run(self.train_step,{\n",
    "                self.states_ph: [s], self.actions_ph: [a], self.rewards_ph: [r], \n",
    "                self.next_states_ph: [next_s], self.is_done_ph: [done]\n",
    "            })\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=0.5 \n",
    "gamma=0.99\n",
    "eta=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/volodymyrkovenko/envs/rl_env/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From <ipython-input-4-5cfd22df9210>:56: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "agent = ApproximateQlearning(state_dim,n_actions,\n",
    "                            epsilon=epsilon, gamma=gamma, eta=eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on cartpole env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(agent, env, train=False, t_max=1000):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        a = agent.get_action(s)       \n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        if train:\n",
    "            agent.update(s,a,r,next_s,done)\n",
    "        total_reward += r\n",
    "        s = next_s\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, epochs=1000,episodes=100,\n",
    "         epsilon_decay=True):\n",
    "    for i in range(epochs):\n",
    "        session_rewards = [generate_session(agent, env, train = True) for _ in range(episodes)]\n",
    "        print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), agent.epsilon))\n",
    "        if epsilon_decay:\n",
    "            agent.epsilon *= 0.99\n",
    "        assert epsilon >= 1e-4, \"Make sure epsilon is always nonzero during training\"\n",
    "\n",
    "        if np.mean(session_rewards) > 300:\n",
    "            print(\"You Win!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0\tmean reward = 14.610\tepsilon = 0.500\n",
      "epoch #1\tmean reward = 14.880\tepsilon = 0.495\n",
      "epoch #2\tmean reward = 13.520\tepsilon = 0.490\n",
      "epoch #3\tmean reward = 15.300\tepsilon = 0.485\n",
      "epoch #4\tmean reward = 13.820\tepsilon = 0.480\n",
      "epoch #5\tmean reward = 15.570\tepsilon = 0.475\n",
      "epoch #6\tmean reward = 16.100\tepsilon = 0.471\n",
      "epoch #7\tmean reward = 18.370\tepsilon = 0.466\n",
      "epoch #8\tmean reward = 26.640\tepsilon = 0.461\n",
      "epoch #9\tmean reward = 41.120\tepsilon = 0.457\n",
      "epoch #10\tmean reward = 52.230\tepsilon = 0.452\n",
      "epoch #11\tmean reward = 53.950\tepsilon = 0.448\n",
      "epoch #12\tmean reward = 57.650\tepsilon = 0.443\n",
      "epoch #13\tmean reward = 81.040\tepsilon = 0.439\n",
      "epoch #14\tmean reward = 95.630\tepsilon = 0.434\n",
      "epoch #15\tmean reward = 107.400\tepsilon = 0.430\n",
      "epoch #16\tmean reward = 161.080\tepsilon = 0.426\n",
      "epoch #17\tmean reward = 150.970\tepsilon = 0.421\n",
      "epoch #18\tmean reward = 159.820\tepsilon = 0.417\n",
      "epoch #19\tmean reward = 205.860\tepsilon = 0.413\n",
      "epoch #20\tmean reward = 219.820\tepsilon = 0.409\n",
      "epoch #21\tmean reward = 209.520\tepsilon = 0.405\n",
      "epoch #22\tmean reward = 255.690\tepsilon = 0.401\n",
      "epoch #23\tmean reward = 298.370\tepsilon = 0.397\n",
      "epoch #24\tmean reward = 431.290\tepsilon = 0.393\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "train(agent,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showing how agent performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"Step: %d %s\" % (step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of rewards : 50.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "steps = 50\n",
    "s = env.reset()\n",
    "rewards = 0\n",
    "for t in range(steps):\n",
    "    show_state(env,t)\n",
    "    env.render()\n",
    "    a = agent.get_action(s)\n",
    "    next_s, r, done, _ = env.step(a)\n",
    "    rewards+=r\n",
    "    s = next_s\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        break\n",
    "env.close()\n",
    "display.clear_output(wait=True)\n",
    "print(\"Sum of rewards : {}\".format(rewards))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
