{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating playing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1387bf4d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "if hasattr(env, '_max_episode_steps'):\n",
    "    env = env.env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceAgent:\n",
    "    def __init__(self, state_dim, n_actions,alpha=0.1):\n",
    "        self.state_dim = state_dim\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.ph_states, self.ph_actions, self.ph_cumulative_rewards = self.init_placeholders()\n",
    "        self.policy, self.log_policy  = self.make_model()\n",
    "        self.update = self.init_graph()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def make_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(InputLayer(input_shape=self.state_dim))\n",
    "        model.add(Dense(units=200,activation='relu'))\n",
    "        model.add(Dense(units=150,activation='relu'))\n",
    "        model.add(Dense(units=75,activation='relu'))\n",
    "        model.add(Dense(units=self.n_actions))\n",
    "        logits = model(self.ph_states) \n",
    "        policy = tf.nn.softmax(logits)\n",
    "        log_policy = tf.nn.log_softmax(logits)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        return policy, log_policy \n",
    "    \n",
    "    def init_placeholders(self):\n",
    "        ph_states = tf.placeholder('float32', (None,) + self.state_dim, name=\"states\")\n",
    "        ph_actions = tf.placeholder('int32', name=\"action_ids\")\n",
    "        ph_cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")\n",
    "        return ph_states, ph_actions, ph_cumulative_rewards\n",
    "    \n",
    "    def init_graph(self):\n",
    "        indices = tf.stack([tf.range(tf.shape(self.log_policy)[0]), self.ph_actions], axis=-1)\n",
    "        log_policy_for_actions = tf.gather_nd(self.log_policy, indices)\n",
    "        J = tf.reduce_mean(log_policy_for_actions*self.ph_cumulative_rewards,axis=-1)\n",
    "        entropy = - tf.reduce_mean(self.policy*self.log_policy)\n",
    "        loss = -(J + self.alpha * entropy)\n",
    "        update = tf.train.AdamOptimizer().minimize(loss)\n",
    "        return update\n",
    "    \n",
    "    def train(self, states, actions, cumulative_rewards):\n",
    "         self.update.run({\n",
    "                self.ph_states: states,\n",
    "                self.ph_actions: actions,\n",
    "                self.ph_cumulative_rewards: cumulative_rewards,\n",
    "                        })\n",
    "    \n",
    "    def predict(self, s):\n",
    "        probs = self.policy.eval({self.ph_states: [s]})[0]\n",
    "        action = np.random.choice(a=self.n_actions,p=probs)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/volodymyrkovenko/envs/rl_env/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "agent = ReinforceAgent(n_actions=n_actions,state_dim=state_dim,alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(agent, env, t_max=1000):\n",
    "    \"\"\" \n",
    "    Play a full session with REINFORCE agent.\n",
    "    Returns sequences of states, actions, and rewards.\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        a = agent.predict(s)\n",
    "        new_s, r, done, info = env.step(a)\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards,  \n",
    "                           gamma=0.99  \n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Take a list of immediate rewards r(s,a) for the whole session \n",
    "    and compute cumulative returns \n",
    "    \"\"\"\n",
    "    cum_rewards = [rewards[-1]]\n",
    "    for t, r in enumerate(rewards[::-1]):\n",
    "        g_t = r+gamma*cum_rewards[t]\n",
    "        cum_rewards.append(g_t)\n",
    "    return cum_rewards[1:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent,env, gamma=0.99, num_epochs=100, ep_per_epoch=100):\n",
    "    for epoch in range(num_epochs):\n",
    "        global_rewards = []\n",
    "        for episode in range(ep_per_epoch):\n",
    "            states, actions, rewards = generate_session(agent, env)\n",
    "            returns = get_cumulative_rewards(rewards,gamma)\n",
    "            agent.train(states,actions,returns)\n",
    "            global_rewards.append(sum(rewards))\n",
    "            \n",
    "\n",
    "        print(\"mean reward: %.3f\" % (np.mean(global_rewards)))\n",
    "        if np.mean(global_rewards) > 300:\n",
    "            print(\"You Win!\")  # \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: 175.860\n",
      "mean reward: 90.790\n",
      "mean reward: 93.710\n",
      "mean reward: 114.130\n",
      "mean reward: 90.010\n",
      "mean reward: 151.560\n",
      "mean reward: 595.890\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "train(agent,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
